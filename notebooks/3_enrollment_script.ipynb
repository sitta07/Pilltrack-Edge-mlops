{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwslosZHfO2J",
        "outputId": "68abf7d3-d394-482b-fa67-13b29c77970c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.40.75-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Collecting botocore<1.41.0,>=1.40.75 (from boto3)\n",
            "  Downloading botocore-1.40.75-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.75->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.75->boto3) (2.5.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.75-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.75-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, faiss-cpu, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.40.75 botocore-1.40.75 faiss-cpu-1.13.0 jmespath-1.0.1 s3transfer-0.14.0\n",
            "‚úÖ Libraries Installed!\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 1: Install Dependencies ---\n",
        "!pip install faiss-cpu boto3 tensorflow tqdm pillow\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# ‡πÄ‡∏ä‡πá‡∏Ñ GPU ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Enrollment (CPU ‡∏Å‡πá‡πÄ‡∏£‡πá‡∏ß‡∏û‡∏≠)\n",
        "print(\"‚úÖ Libraries Installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "# 1.1 ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1.2 ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÅ‡∏•‡∏∞ Path ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á\n",
        "zip_path = '/content/drive/MyDrive/dataset/pills_dataset_resnet.zip'\n",
        "extract_dir = '/content/pill_dataset/'\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "print(f\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å: {zip_path}\")\n",
        "\n",
        "# 1.3 ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå ZIP\n",
        "if zipfile.is_zipfile(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "        print(f\"‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‚Üí {extract_dir}\")\n",
        "else:\n",
        "    print(\"‚ùå ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ZIP ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY_Rn1lEijBM",
        "outputId": "da47707a-f725-466a-aad2-fc43a9198aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å: /content/drive/MyDrive/dataset/pills_dataset_resnet.zip\n",
            "‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‚Üí /content/pill_dataset/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2: Download TFLite from S3 ---\n",
        "import boto3\n",
        "import os\n",
        "\n",
        "# 1. (AWS Credentials)\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAQH25QL54Z3AKVQ4B\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
        "\n",
        "# 2. Config Path\n",
        "BUCKET_NAME = \"mlflow-bucket-pilltrackml\"\n",
        "# Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå TFLite ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡πÉ‡∏ô Notebook ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
        "S3_MODEL_KEY = \"production/edge-models/v1/student_model.tflite\"\n",
        "LOCAL_MODEL_PATH = \"student_model.tflite\"\n",
        "\n",
        "# 3. Download\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "if not os.path.exists(LOCAL_MODEL_PATH):\n",
        "    print(f\"‚¨áÔ∏è Downloading model from s3://{BUCKET_NAME}/{S3_MODEL_KEY} ...\")\n",
        "    try:\n",
        "        s3.download_file(BUCKET_NAME, S3_MODEL_KEY, LOCAL_MODEL_PATH)\n",
        "        print(f\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {LOCAL_MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    print(\"‚úÖ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP3p5Wxugj_g",
        "outputId": "e87b9908-20ea-4306-bf50-ee7ce31bfe79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading model from s3://mlflow-bucket-pilltrackml/production/edge-models/v1/student_model.tflite ...\n",
            "‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: student_model.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 3: Initialize TFLite Interpreter ---\n",
        "import tensorflow.lite as tflite\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Config Path (‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤)\n",
        "LOCAL_MODEL_PATH = \"student_model.tflite\"\n",
        "\n",
        "if not os.path.exists(LOCAL_MODEL_PATH):\n",
        "    print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {LOCAL_MODEL_PATH} ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô Cell 2 ‡πÉ‡∏´‡∏°‡πà\")\n",
        "else:\n",
        "    print(\"‚öôÔ∏è Loading Interpreter...\")\n",
        "    interpreter = tflite.Interpreter(model_path=LOCAL_MODEL_PATH)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Input/Output ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    input_shape = input_details[0]['shape'] # ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô [1, 224, 224, 3]\n",
        "    input_dtype = input_details[0]['dtype'] # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô float32 ‡∏´‡∏£‡∏∑‡∏≠ int8\n",
        "\n",
        "    # ‡∏Ñ‡πà‡∏≤ Mean/Std ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ImageNet Normalization (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏°‡∏≤)\n",
        "    IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "    IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "    print(f\"‚úÖ Model Loaded Successfully!\")\n",
        "    print(f\"   - Input Shape: {input_shape}\")\n",
        "    print(f\"   - Input Type: {input_dtype}\")\n",
        "\n",
        "    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏¢‡∏≤ -> ‡∏£‡∏´‡∏±‡∏™‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Vector)\n",
        "    def get_embedding(image_path):\n",
        "        # 1. Load & Resize\n",
        "        try:\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "            img = img.resize((input_shape[1], input_shape[2])) # 224x224\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Read Error: {e}\")\n",
        "            return None\n",
        "\n",
        "        # 2. Preprocessing (‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô Train ‡πÄ‡∏õ‡πä‡∏∞‡πÜ)\n",
        "        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô 0-1\n",
        "        input_data = np.array(img, dtype=np.float32) / 255.0\n",
        "\n",
        "        # ‡∏ó‡∏≥ Normalize (Standard ImageNet)\n",
        "        # ‡∏™‡∏π‡∏ï‡∏£: (value - mean) / std\n",
        "        input_data = (input_data - IMAGENET_MEAN) / IMAGENET_STD\n",
        "\n",
        "        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Batch Dimension: (224, 224, 3) -> (1, 224, 224, 3)\n",
        "        input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "        # 3. Handle Int8 Quantization (‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô Int8 ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ input ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡∏ï‡∏ä‡πå)\n",
        "        if input_dtype == np.int8:\n",
        "            scale, zero_point = input_details[0]['quantization']\n",
        "            # ‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏õ‡∏•‡∏á Float -> Int8: (value / scale) + zero_point\n",
        "            input_data = (input_data / scale + zero_point).astype(np.int8)\n",
        "\n",
        "        # 4. Run Inference (‡∏™‡∏±‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # 5. Get Output Vector\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 1D Array (Vector ‡∏¢‡∏≤‡∏ß‡πÜ)\n",
        "        return output_data.flatten()\n",
        "\n",
        "    print(\"‚úÖ Function 'get_embedding' is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Q5fpLTiPoq",
        "outputId": "29a1a9a5-cbf7-4453-be24-e82e448bbf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Loading Interpreter...\n",
            "‚úÖ Model Loaded Successfully!\n",
            "   - Input Shape: [  1 224 224   3]\n",
            "   - Input Type: <class 'numpy.int8'>\n",
            "‚úÖ Function 'get_embedding' is ready!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 4: Build FAISS Index ---\n",
        "import faiss\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Config Path (‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà Unzip ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Notebook 1)\n",
        "DATASET_ROOT = '/content/pill_dataset/pills_dataset_resnet/train'\n",
        "INDEX_OUTPUT = \"pill_db.index\"\n",
        "LABELS_OUTPUT = \"labels.json\"\n",
        "\n",
        "# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "embeddings = []\n",
        "labels_map = {} # Map ID (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç) -> ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤ (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
        "current_id = 0\n",
        "\n",
        "print(f\"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‡∏à‡∏≤‡∏Å: {DATASET_ROOT}\")\n",
        "\n",
        "# 1. ‡∏´‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏¢‡∏≤ (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ñ‡∏∑‡∏≠ 1 ‡∏ä‡∏ô‡∏¥‡∏î)\n",
        "pill_classes = sorted(glob(os.path.join(DATASET_ROOT, '*')))\n",
        "print(f\"   - ‡∏û‡∏ö‡∏¢‡∏≤ {len(pill_classes)} ‡∏ä‡∏ô‡∏¥‡∏î\")\n",
        "\n",
        "# 2. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏ô‡∏¥‡∏î‡∏¢‡∏≤\n",
        "for folder in tqdm(pill_classes):\n",
        "    if not os.path.isdir(folder): continue\n",
        "\n",
        "    pill_name = os.path.basename(folder)\n",
        "\n",
        "    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\n",
        "    images = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "        images.extend(glob(os.path.join(folder, ext)))\n",
        "\n",
        "    # ‚ö†Ô∏è Optimization: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 20-30 ‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏¢‡∏≤ ‡∏Å‡πá‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Reference (Enrollment)\n",
        "    # ‡∏Å‡∏≤‡∏£‡∏°‡∏µ Reference ‡πÄ‡∏¢‡∏≠‡∏∞‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ Index ‡∏à‡∏∞‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏•‡∏∞‡∏Å‡∏¥‡∏ô Ram\n",
        "    selected_images = images[:30]\n",
        "\n",
        "    for img_path in selected_images:\n",
        "        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å Cell 3 ‡∏°‡∏≤‡∏™‡∏Å‡∏±‡∏î Vector\n",
        "        vector = get_embedding(img_path)\n",
        "\n",
        "        if vector is not None:\n",
        "            embeddings.append(vector)\n",
        "\n",
        "            # ‡∏à‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ß‡πà‡∏≤ ID ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏¢‡∏≤‡∏≠‡∏∞‡πÑ‡∏£\n",
        "            # FAISS ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô ID 0, 1, 2... ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡∏ß‡πà‡∏≤‡πÄ‡∏•‡∏Ç‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏¢‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£\n",
        "            labels_map[current_id] = pill_name\n",
        "            current_id += 1\n",
        "\n",
        "# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å FAISS Index\n",
        "if len(embeddings) > 0:\n",
        "    # ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Numpy Matrix (Float32) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
        "    embeddings_matrix = np.array(embeddings).astype('float32')\n",
        "    dimension = embeddings_matrix.shape[1] # ‡∏Ç‡∏ô‡∏≤‡∏î Vector (‡πÄ‡∏ä‡πà‡∏ô 1280 ‡∏à‡∏≤‡∏Å MobileNetV2)\n",
        "\n",
        "    print(f\"\\nüìä Creating FAISS Index (Dimension: {dimension}, Total Vectors: {len(embeddings)})\")\n",
        "\n",
        "    # ‡πÉ‡∏ä‡πâ IndexFlatL2 (Euclidean Distance) -> ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏û‡∏±‡∏ô\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings_matrix)\n",
        "\n",
        "    # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏á Disk\n",
        "    faiss.write_index(index, INDEX_OUTPUT)\n",
        "    with open(LABELS_OUTPUT, 'w') as f:\n",
        "        json.dump(labels_map, f, indent=2) # indent=2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢\n",
        "\n",
        "    print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
        "    print(f\"   üíæ Saved Index: {INDEX_OUTPUT}\")\n",
        "    print(f\"   üíæ Saved Labels: {LABELS_OUTPUT}\")\n",
        "else:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏¢‡∏≤‡πÄ‡∏•‡∏¢! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ Path Dataset ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jlgwDvgiUIS",
        "outputId": "0bcd4c64-2d78-4521-a7c8-44dd029ded92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‡∏à‡∏≤‡∏Å: /content/pill_dataset/pills_dataset_resnet/train\n",
            "   - ‡∏û‡∏ö‡∏¢‡∏≤ 46 ‡∏ä‡∏ô‡∏¥‡∏î\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:33<00:00,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Creating FAISS Index (Dimension: 46, Total Vectors: 1380)\n",
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
            "   üíæ Saved Index: pill_db.index\n",
            "   üíæ Saved Labels: labels.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 5: Upload Artifacts to S3 ---\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏ö‡∏ô S3 (‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Model ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏´‡∏≤‡∏á‡πà‡∏≤‡∏¢)\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠ Bucket ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö\n",
        "S3_INDEX_KEY = \"production/edge-models/v1/pill_db.index\"\n",
        "S3_LABELS_KEY = \"production/edge-models/v1/labels.json\"\n",
        "\n",
        "print(\"üöÄ Uploading Index & Labels to S3...\")\n",
        "\n",
        "try:\n",
        "    # Upload Index\n",
        "    s3.upload_file(INDEX_OUTPUT, BUCKET_NAME, S3_INDEX_KEY)\n",
        "    print(f\"‚úÖ Upload Index ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: s3://{BUCKET_NAME}/{S3_INDEX_KEY}\")\n",
        "\n",
        "    # Upload Labels\n",
        "    s3.upload_file(LABELS_OUTPUT, BUCKET_NAME, S3_LABELS_KEY)\n",
        "    print(f\"‚úÖ Upload Labels ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: s3://{BUCKET_NAME}/{S3_LABELS_KEY}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Upload Failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP7-1cPvj3tX",
        "outputId": "d2476ffc-b8dd-4324-a6df-64b2a40bfd43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Uploading Index & Labels to S3...\n",
            "‚úÖ Upload Index ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: s3://mlflow-bucket-pilltrackml/production/edge-models/v1/pill_db.index\n",
            "‚úÖ Upload Labels ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: s3://mlflow-bucket-pilltrackml/production/edge-models/v1/labels.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 6: Generate Metadata & Final Upload ---\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import boto3\n",
        "\n",
        "# --- CONFIG ---\n",
        "BUCKET_NAME = \"mlflow-bucket-pilltrackml\"\n",
        "S3_PREFIX = \"production/edge-models/v1/\" # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á\n",
        "\n",
        "# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö (Local Filename -> S3 Key)\n",
        "# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Colab\n",
        "FILES_MAP = {\n",
        "    \"student_model.tflite\": \"student_model.tflite\",\n",
        "    \"pill_db.index\": \"pill_db.index\",\n",
        "    \"labels.json\": \"labels.json\",\n",
        "    # YOLO ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å S3 ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤\n",
        "    \"yolo_detector.tflite\": \"models/yolo/yolo_detector.tflite\" # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ path ‡∏ô‡∏µ‡πâ‡∏ö‡∏ô S3\n",
        "}\n",
        "\n",
        "# ‡∏ä‡∏∑‡πà‡∏≠ Key ‡πÉ‡∏ô Metadata ‡∏ó‡∏µ‡πà verify_model.py ‡∏£‡∏≠‡∏ï‡∏£‡∏ß‡∏à\n",
        "HASH_KEYS = {\n",
        "    \"student_model.tflite\": \"student_model_md5_hash\",\n",
        "    \"pill_db.index\": \"index_md5_hash\",\n",
        "    \"labels.json\": \"labels_md5_hash\",\n",
        "    \"yolo_detector.tflite\": \"yolo_md5_hash\"\n",
        "}\n",
        "\n",
        "OUTPUT_METADATA = \"model_metadata.json\"\n",
        "\n",
        "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n",
        "s3 = boto3.client('s3')\n",
        "metadata = {\n",
        "    \"version\": \"v1.0-auto\",\n",
        "    \"release_date\": \"2025-11-18\",\n",
        "    \"description\": \"Auto-generated from Colab Pipeline\"\n",
        "}\n",
        "\n",
        "print(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata...\")\n",
        "\n",
        "for filename, s3_path_hint in FILES_MAP.items():\n",
        "    # 1. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (‡πÄ‡∏ä‡πà‡∏ô YOLO) ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å S3\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö {filename} ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô S3...\")\n",
        "        try:\n",
        "            # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ YOLO ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏¢‡∏Å‡∏ó‡∏µ‡πà ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡πá‡πÅ‡∏Å‡πâ path ‡πÑ‡∏î‡πâ\n",
        "            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ‡∏ú‡∏°‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ß‡∏≤‡∏á YOLO ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà production/edge-models/v1/yolo_detector.tflite ‡πÅ‡∏•‡πâ‡∏ß\n",
        "            # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢ ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î YOLO ‡πÉ‡∏™‡πà Colab ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏±‡∏ô Cell ‡∏ô‡∏µ‡πâ\n",
        "            s3_key_to_download = f\"{S3_PREFIX}{filename}\"\n",
        "\n",
        "            # *‡∏Å‡∏£‡∏ì‡∏µ YOLO ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ s3_key_to_download ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö*\n",
        "            # s3_key_to_download = \"models/base/yolo_detector.tflite\"\n",
        "\n",
        "            s3.download_file(BUCKET_NAME, s3_key_to_download, filename)\n",
        "            print(f\"   ‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠/‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}\")\n",
        "            print(f\"   üëâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå '{filename}' ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô Colab Files ‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡∏°‡∏∑‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö\")\n",
        "            continue\n",
        "\n",
        "    # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Hash\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, \"rb\") as f:\n",
        "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "        key_name = HASH_KEYS.get(filename, f\"{filename}_hash\")\n",
        "        metadata[key_name] = file_hash\n",
        "        print(f\"‚úÖ Hash {filename}: {file_hash[:10]}...\")\n",
        "\n",
        "# 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Metadata\n",
        "with open(OUTPUT_METADATA, \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"\\nüìÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå {OUTPUT_METADATA} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")\n",
        "\n",
        "# 4. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Metadata ‡∏Ç‡∏∂‡πâ‡∏ô S3\n",
        "S3_META_KEY = f\"{S3_PREFIX}{OUTPUT_METADATA}\"\n",
        "print(f\"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Metadata ‡πÑ‡∏õ‡∏ó‡∏µ‡πà s3://{BUCKET_NAME}/{S3_META_KEY} ...\")\n",
        "\n",
        "try:\n",
        "    s3.upload_file(OUTPUT_METADATA, BUCKET_NAME, S3_META_KEY)\n",
        "    print(\"üéâüéâüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏° Deploy ‡πÅ‡∏•‡πâ‡∏ß! üéâüéâüéâ\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i09CKOt8mDUv",
        "outputId": "5b829ecf-338c-4689-83ba-f17f753b2ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata...\n",
            "‚úÖ Hash student_model.tflite: a1549201e7...\n",
            "‚úÖ Hash pill_db.index: e224eb2e79...\n",
            "‚úÖ Hash labels.json: 2d431afa2e...\n",
            "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö yolo_detector.tflite ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô S3...\n",
            "   ‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î yolo_detector.tflite ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
            "‚úÖ Hash yolo_detector.tflite: c776fead9a...\n",
            "\n",
            "üìÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå model_metadata.json ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\n",
            "üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Metadata ‡πÑ‡∏õ‡∏ó‡∏µ‡πà s3://mlflow-bucket-pilltrackml/production/edge-models/v1/model_metadata.json ...\n",
            "üéâüéâüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏° Deploy ‡πÅ‡∏•‡πâ‡∏ß! üéâüéâüéâ\n"
          ]
        }
      ]
    }
  ]
}